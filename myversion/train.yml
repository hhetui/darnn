# 所有模型参数按照文件名添加在下面

ver2: #LSTM模型
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 32
    drop_ratio: 0
 
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver2_1: #LSTM模型改变隐向量长度
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 64
    drop_ratio: 0
 
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.01
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver3: #LSTM的EN-DN结构
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver4: #CNN 结合 self-attention
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver5: #DARNN 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver5_1: #DARNN 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 64
    decoder_num_hidden: 64
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver6: #DARNN+self-attention 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: True
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_1: #增加num_hidden
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 64
    decoder_num_hidden: 64
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_2: #增加num_hidden
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_3: #T改为30
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30
ver6_4: #T改为10
  #model settings
  model_conf:
    input_size: 6
    T: 10
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_10day'
    T: 10
ver6_5: #DARNN+self-attention带dropout独立版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0.1
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: True
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver7: #darnn中替换成self-attention
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver7_1: #self_attention写成模块，另外加入dropout测试
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0.1
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver7_2: #self_attention写成模块(带dropout)，最后使用resdual
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0.1
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver8: #darnn中替换成self-attention
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver9: #DARNN内部使用CNNattention+self-attention 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.01
    weight_decay: 0.0
    patience: 50
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver9_1: #DARNN内部使用CNNattention+self-attention （师兄理解）参数共享
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.01
    weight_decay: 0.0
    patience: 50
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
    

ver10: #DARNN+self-attention使用师兄第二篇CNN down-sampling模型过拟合 增加正则化项
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0
    patience: 2000
    stop: 1000
    resume: True
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30  
ver10_1: #增加dropout层
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0.3
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30
ver10_2: #减少hidden_size
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30