# 所有模型参数
ver2: #LSTM模型
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 32
    drop_ratio: 0
 
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver2_1: #LSTM模型改变隐向量长度
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 64
    drop_ratio: 0
 
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.01
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver3: #LSTM的EN-DN结构
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver4: #CNN 结合 self-attention
  #model settings
  model_conf:
    input_size: 6
    time_step: 20
    hidden_size: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver5: #DARNN 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver5_1: #DARNN 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 64
    decoder_num_hidden: 64
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver6: #DARNN+self-attention 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: True
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_1: #增加num_hidden
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 64
    decoder_num_hidden: 64
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_2: #增加num_hidden
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20
ver6_3: #T改为30
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30
ver6_4: #T改为10
  #model settings
  model_conf:
    input_size: 6
    T: 10
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_10day'
    T: 10


ver7: #darnn中替换成self-attention
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver8: #darnn中替换成self-attention
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.0
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver9: #DARNN内部使用CNNattention+self-attention 基础版
  #model settings
  model_conf:
    input_size: 6
    T: 20
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.01
    patience: 50
    stop: 1000
    resume: True
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_20day'
    T: 20

ver10: #DARNN+self-attention使用师兄第二篇CNN down-sampling模型过拟合 增加正则化项
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.01
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30
  
ver10_1: #增加dropout层
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 128
    decoder_num_hidden: 128
    drop_ratio: 0.3
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.01
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30

ver10_2: #减少hidden_size
  #model settings
  model_conf:
    input_size: 6
    T: 30
    encoder_num_hidden: 32
    decoder_num_hidden: 32
    drop_ratio: 0
  
  #train settings
  train_conf:
    epoch: 1500
    batch: 256
    split: 30
    learning_rate: 0.001
    weight_decay: 0.01
    patience: 2000
    stop: 1000
    resume: False
    checkpoint_path: '../result/'
    log_file: 'trainer.log'

  #dataset settings
  data_conf:
    train_list: [2010,2011,2012,2013,2014,2015,2016,2017,2018]
    test_list: [123,456,789,1012]
    datapath: '../v_30day'
    T: 30